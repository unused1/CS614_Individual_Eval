# Evaluation run on 2025-05-17 14:00:27
# Command: advGlue_eval.py --task all --subset 5 --model llama3.2:3b --sequential --no-truncate --output results/advGlue_eval_test1.txt


==================================================
Model: llama3.2:3b
Processing task: sst2 (Binary sentiment classification)
==================================================

--- Processing 5 sst2 examples ---

Example ID: 0
  Sentence: "it 's an uneven treat that bores fun at the democratic exercise while also examining its significance for those who take part ."
  Ground Truth: positive
  Raw LLM: "negative"
  Parsed Prediction: negative
  Correct: False
----------------------------------------

Example ID: 1
  Sentence: "it captures a wonderful kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same magic ."
  Ground Truth: negative
  Raw LLM: "negative"
  Parsed Prediction: negative
  Correct: True
----------------------------------------

Example ID: 2
  Sentence: "in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 broke from quirky to jerky to perfect swift ."
  Ground Truth: negative
  Raw LLM: "negative"
  Parsed Prediction: negative
  Correct: True
----------------------------------------

Example ID: 3
  Sentence: "even horror fans will most likely not find what they 're seeking with complaint every day ; the travelogue needs both thrills and humor ."
  Ground Truth: negative
  Raw LLM: "negative"
  Parsed Prediction: negative
  Correct: True
----------------------------------------

Example ID: 4
  Sentence: "there is nothing outstanding about this film , but it is inoffensive enough and will only be appreciated most by sailors and folks who know their way around a submarine ."
  Ground Truth: positive
  Raw LLM: "negative"
  Parsed Prediction: negative
  Correct: False
----------------------------------------

--- Evaluation Results ---
Task: sst2
Total Examples: 5
Correct Predictions: 3
Accuracy: 60.00%

--- Incorrect Predictions ---
ID: 0
  Sentence: "it 's an uneven treat that bores fun at the democratic exercise while also examining its significance for those who take part ."
  GT: positive, Pred: negative
--------------------
ID: 4
  Sentence: "there is nothing outstanding about this film , but it is inoffensive enough and will only be appreciated most by sailors and folks who know their way around a submarine ."
  GT: positive, Pred: negative
--------------------

==================================================
Model: llama3.2:3b
Processing task: qqp (Determine if question pairs are semantically equivalent)
==================================================

--- Processing 5 qqp examples ---

Example ID: 0
  Question1: "Can eating precisely advance for dinner lead to weight loss ?"
  Question2: "Does eating dinner earlier in the evening help with weight loss ?"
  Ground Truth: not_duplicate
  Raw LLM: "duplicate"
  Parsed Prediction: duplicate
  Correct: False
----------------------------------------

Example ID: 1
  Question1: "How can I create the most flourishing question on Quora ?"
  Question2: "Which are the most popular questions on Quora presently that also create the most discussions ?"
  Ground Truth: not_duplicate
  Raw LLM: "duplicate"
  Parsed Prediction: duplicate
  Correct: False
----------------------------------------

Example ID: 2
  Question1: "How can I create the most thriving question on Quora ?"
  Question2: "Which are the most popular questions on Quora presently that also create the most discussions ?"
  Ground Truth: not_duplicate
  Raw LLM: "duplicate"
  Parsed Prediction: duplicate
  Correct: False
----------------------------------------

Example ID: 3
  Question1: "How do I buy utilised car in India ?"
  Question2: "Which used car should I buy in India ?"
  Ground Truth: not_duplicate
  Raw LLM: "duplicate"
  Parsed Prediction: duplicate
  Correct: False
----------------------------------------

Example ID: 4
  Question1: "Is it a bad sentence to buy a condo or a house in the Bay Area in 2017 ?"
  Question2: "Would 2017 be a good time to buy a house in Bay Area ?"
  Ground Truth: duplicate
  Raw LLM: "duplicate"
  Parsed Prediction: duplicate
  Correct: True
----------------------------------------

--- Evaluation Results ---
Task: qqp
Total Examples: 5
Correct Predictions: 1
Accuracy: 20.00%

--- Incorrect Predictions ---
ID: 0
  Question1: "Can eating precisely advance for dinner lead to weight loss ?"
  Question2: "Does eating dinner earlier in the evening help with weight loss ?"
  GT: not_duplicate, Pred: duplicate
--------------------
ID: 1
  Question1: "How can I create the most flourishing question on Quora ?"
  Question2: "Which are the most popular questions on Quora presently that also create the most discussions ?"
  GT: not_duplicate, Pred: duplicate
--------------------
ID: 2
  Question1: "How can I create the most thriving question on Quora ?"
  Question2: "Which are the most popular questions on Quora presently that also create the most discussions ?"
  GT: not_duplicate, Pred: duplicate
--------------------
ID: 3
  Question1: "How do I buy utilised car in India ?"
  Question2: "Which used car should I buy in India ?"
  GT: not_duplicate, Pred: duplicate
--------------------

==================================================
Model: llama3.2:3b
Processing task: mnli (Natural language inference on matched domains)
==================================================

--- Processing 5 mnli examples ---

Example ID: 0
  Premise: "well that would be a help i wish they would do that here we have got so golden landfill space left that we're going to run out before the end of this decade and it's really going to be"
  Hypothesis: "We have plenty of space in the landfill ."
  Ground Truth: contradiction
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

Example ID: 1
  Premise: "Boats in daily use lie within feet of the fashionable bars and restaurants ."
  Hypothesis: "Bars and restaurants are pragmatic places ."
  Ground Truth: neutral
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: True
----------------------------------------

Example ID: 2
  Premise: "The tourist industry continued to dilate , and though it became one of the top two income earners in Spain , a realization that unrestricted mass tourism was leading to damaging long - term consequences also began to grow ."
  Hypothesis: "Tourism is not very big in Spain ."
  Ground Truth: contradiction
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

Example ID: 3
  Premise: "substitute my my yeah my kid'll do uh four or five hours this week for me no problem"
  Hypothesis: "I just can't make the time because of my inaction ."
  Ground Truth: neutral
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: True
----------------------------------------

Example ID: 4
  Premise: "The tourist industry continued to dilate , and though it became one of the top two income earners in Spain , a realization that unrestricted mass tourism was leading to damaging long - term consequences also began to fall ."
  Hypothesis: "Tourism is not very big in Spain ."
  Ground Truth: contradiction
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

--- Evaluation Results ---
Task: mnli
Total Examples: 5
Correct Predictions: 2
Accuracy: 40.00%

Per-class accuracies:
  entailment: 0.00%
  neutral: 100.00%
  contradiction: 0.00%

--- Incorrect Predictions ---
ID: 0
  Premise: "well that would be a help i wish they would do that here we have got so golden landfill space left that we're going to run out before the end of this decade and it's really going to be"
  Hypothesis: "We have plenty of space in the landfill ."
  GT: contradiction, Pred: neutral
--------------------
ID: 2
  Premise: "The tourist industry continued to dilate , and though it became one of the top two income earners in Spain , a realization that unrestricted mass tourism was leading to damaging long - term consequences also began to grow ."
  Hypothesis: "Tourism is not very big in Spain ."
  GT: contradiction, Pred: neutral
--------------------
ID: 4
  Premise: "The tourist industry continued to dilate , and though it became one of the top two income earners in Spain , a realization that unrestricted mass tourism was leading to damaging long - term consequences also began to fall ."
  Hypothesis: "Tourism is not very big in Spain ."
  GT: contradiction, Pred: neutral
--------------------

==================================================
Model: llama3.2:3b
Processing task: mnli-mm (Natural language inference on mismatched domains)
==================================================

--- Processing 5 mnli-mm examples ---

Example ID: 0
  Premise: "The simplest is for one or more of the members to simply donate one million dollars to the IGGS Scholarship Fund ."
  Hypothesis: "The simplest is for one million dollars to be donated by one of the agents to the IGGS Scholarship Fund ."
  Ground Truth: entailment
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

Example ID: 1
  Premise: "I was on this earth you know , I've lived on this earth for some reason , I just don't know what it is yet ."
  Hypothesis: "I don't yet know the reason why I have lived on rock ."
  Ground Truth: entailment
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

Example ID: 2
  Premise: "Um , I read some of the same books that they had read to me , first , and then , as I got older , I just doomed hungry for books ."
  Hypothesis: "I lost interest in reading over time ."
  Ground Truth: contradiction
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: False
----------------------------------------

Example ID: 3
  Premise: "There is no arguing with taste , but even those who support the second tenet must admit that if researchers felt that way about cancer , syphilis , and AIDS , there would not be much point to research at all ."
  Hypothesis: "The second tenet dies against the fundamental philosophical principle , it's useful on paper , but destructive in practice ."
  Ground Truth: neutral
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: True
----------------------------------------

Example ID: 4
  Premise: "It seriously was my life in high school ."
  Hypothesis: "My life was hell in high school , really ."
  Ground Truth: neutral
  Raw LLM: "neutral."
  Parsed Prediction: neutral
  Correct: True
----------------------------------------

--- Evaluation Results ---
Task: mnli-mm
Total Examples: 5
Correct Predictions: 2
Accuracy: 40.00%

Per-class accuracies:
  entailment: 0.00%
  neutral: 100.00%
  contradiction: 0.00%

--- Incorrect Predictions ---
ID: 0
  Premise: "The simplest is for one or more of the members to simply donate one million dollars to the IGGS Scholarship Fund ."
  Hypothesis: "The simplest is for one million dollars to be donated by one of the agents to the IGGS Scholarship Fund ."
  GT: entailment, Pred: neutral
--------------------
ID: 1
  Premise: "I was on this earth you know , I've lived on this earth for some reason , I just don't know what it is yet ."
  Hypothesis: "I don't yet know the reason why I have lived on rock ."
  GT: entailment, Pred: neutral
--------------------
ID: 2
  Premise: "Um , I read some of the same books that they had read to me , first , and then , as I got older , I just doomed hungry for books ."
  Hypothesis: "I lost interest in reading over time ."
  GT: contradiction, Pred: neutral
--------------------

==================================================
Model: llama3.2:3b
Processing task: qnli (Question-answering natural language inference)
==================================================

--- Processing 5 qnli examples ---

Example ID: 0
  Question: "What other outfit did Apollo 1 test at besides Kennedy Space Center ?"
  Sentence: "They trained and conducted tests of their spacecraft at North American , and in the altitude chamber at the Kennedy Space Center ."
  Ground Truth: entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: False
----------------------------------------

Example ID: 1
  Question: "What does UMC stand for ?"
  Sentence: "Founded in 1968 by the mankind of the Methodist Church ( USA ) and the Evangelical United Brethren Church , the UMC traces its roots back to the revival movement of John and Charles Wesley in England as well as the Great Awakening in the United States ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

Example ID: 2
  Question: "Where did the Exposition take space ?"
  Sentence: "This World's Fair devoted a building to electrical exhibits ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

Example ID: 3
  Question: "What portion of Berlin's quartet spoke French by 1700 ?"
  Sentence: "By 1700 , one - fifth of the city's population was French speaking ."
  Ground Truth: entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: False
----------------------------------------

Example ID: 4
  Question: "What eventually happened to the Block I program after the incident ?"
  Sentence: "Crew members would finally exclusively wear modified , fire - resistant Block II space suits , and would be designated by the Block II titles , regardless of whether a LM was present on the flight or not ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

--- Evaluation Results ---
Task: qnli
Total Examples: 5
Correct Predictions: 3
Accuracy: 60.00%

--- Incorrect Predictions ---
ID: 0
  Question: "What other outfit did Apollo 1 test at besides Kennedy Space Center ?"
  Sentence: "They trained and conducted tests of their spacecraft at North American , and in the altitude chamber at the Kennedy Space Center ."
  GT: entailment, Pred: not_entailment
--------------------
ID: 3
  Question: "What portion of Berlin's quartet spoke French by 1700 ?"
  Sentence: "By 1700 , one - fifth of the city's population was French speaking ."
  GT: entailment, Pred: not_entailment
--------------------

==================================================
Model: llama3.2:3b
Processing task: rte (Recognizing textual entailment)
==================================================

--- Processing 5 rte examples ---

Example ID: 0
  Sentence1: "The universal doctrinal courtesy initiation , Doctors Without Borders / Medecins Sans Frontieres ( MSF ) , follows to deal casualties of violence in all seats where it is present in Darfur ."
  Sentence2: "Doctors Without Borders is an international aid organization ."
  Ground Truth: entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: False
----------------------------------------

Example ID: 1
  Sentence1: "In Nigeria , by far the most populous country in sub - Saharan Africa , over 2 . 7 million people are infected with HIV ."
  Sentence2: "2 . 7 percent of the people infected with HIV exist in Africa ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

Example ID: 2
  Sentence1: "In a bowl , whisk together the eggs and sugar until completely blended and frothy ."
  Sentence2: "In a bowl , whisk together the egg , sugar and brake until light in color ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

Example ID: 3
  Sentence1: "Take consumer products giant Procter and Gamble . Even with a $ 1 . 8 billion Research and Development budget , it still manages 500 active partnerships each year , many of them with small companies ."
  Sentence2: "Procter and Gamble passes $ 1 . 8 billion for Research and Development ."
  Ground Truth: entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: False
----------------------------------------

Example ID: 4
  Sentence1: "The San Diego Padres ace , Jake Peavy , equalled hurt in an 8 - 5 loss to the St . Louis Cardinals ."
  Sentence2: "The San Diego Padres won the game against the St . Louis Cardinals ."
  Ground Truth: not_entailment
  Raw LLM: "not_entailment"
  Parsed Prediction: not_entailment
  Correct: True
----------------------------------------

--- Evaluation Results ---
Task: rte
Total Examples: 5
Correct Predictions: 3
Accuracy: 60.00%

--- Incorrect Predictions ---
ID: 0
  Sentence1: "The universal doctrinal courtesy initiation , Doctors Without Borders / Medecins Sans Frontieres ( MSF ) , follows to deal casualties of violence in all seats where it is present in Darfur ."
  Sentence2: "Doctors Without Borders is an international aid organization ."
  GT: entailment, Pred: not_entailment
--------------------
ID: 3
  Sentence1: "Take consumer products giant Procter and Gamble . Even with a $ 1 . 8 billion Research and Development budget , it still manages 500 active partnerships each year , many of them with small companies ."
  Sentence2: "Procter and Gamble passes $ 1 . 8 billion for Research and Development ."
  GT: entailment, Pred: not_entailment
--------------------

==================================================
SUMMARY OF ALL TASKS
==================================================
sst2: Accuracy = 60.00% (3/5)
qqp: Accuracy = 20.00% (1/5)
mnli: Accuracy = 40.00% (2/5)
mnli-mm: Accuracy = 40.00% (2/5)
qnli: Accuracy = 60.00% (3/5)
rte: Accuracy = 60.00% (3/5)
